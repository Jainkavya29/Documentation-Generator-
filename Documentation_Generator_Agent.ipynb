{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCUaq92y-Uo9",
        "outputId": "7383ee10-90a8-44ef-9447-87b180efd6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install nltk pdfplumber pypdf python-docx fpdf transformers requests -q\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from google.colab import files\n",
        "import pdfplumber\n",
        "import pypdf\n",
        "from docx import Document\n",
        "from fpdf import FPDF\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_scaledown_api(prompt_text, context_text=\"\", api_key=\"s2ooL68SevahKqZrYiKwy8ijOMxqoP4Yag4LFzIA\"):\n",
        "    \"\"\"\n",
        "    Calls the ScaleDown API to compress the text.\n",
        "    Returns the compressed prompt.\n",
        "    \"\"\"\n",
        "    url = \"https://api.scaledown.xyz/compress/raw/\"\n",
        "    headers = {\n",
        "        'x-api-key': api_key,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    payload = {\n",
        "        \"context\": context_text,\n",
        "        \"prompt\": prompt_text,\n",
        "        \"scaledown\": {\"rate\": \"auto\"}\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        if result.get(\"successful\"):\n",
        "            return result.get(\"compressed_prompt\")\n",
        "        else:\n",
        "            raise Exception(\"ScaleDown API failed:\", result)\n",
        "    else:\n",
        "        raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n"
      ],
      "metadata": {
        "id": "nhbF1EePR9jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ScaleDown Compressor wrapper\n",
        "class ScaleDownAPI:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def compress_text(self, text):\n",
        "        context = \"This document is for AI documentation generation\"\n",
        "        try:\n",
        "            compressed = call_scaledown_api(text, context_text=context, api_key=self.api_key)\n",
        "            return compressed\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: ScaleDown Compression failed: {e}\")\n",
        "            return text # Fallback to original text\n",
        "\n",
        "# Structure Extractor\n",
        "class StructureModel:\n",
        "    def extract(self, text):\n",
        "        lines = text.split(\"\\n\")\n",
        "        sections = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.isupper() or line.endswith(\":\"):\n",
        "                sections.append(line)\n",
        "        return sections\n",
        "\n",
        "# Explanation Model (simplify text)\n",
        "class ExplanationModel:\n",
        "    def simplify(self, text):\n",
        "        replacements = {\n",
        "            \"shall\": \"must\",\n",
        "            \"adhere to\": \"follow\",\n",
        "            \"commence\": \"start\",\n",
        "            \"terminate\": \"end\",\n",
        "            \"utilize\": \"use\"\n",
        "        }\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "        return text\n",
        "\n",
        "# Summary Model with Fallback Logic\n",
        "class SummaryModel:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            # Try to load the pipeline\n",
        "            self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                # Fallback attempt to load by model name only\n",
        "                self.summarizer = pipeline(model=\"facebook/bart-large-cnn\")\n",
        "            except:\n",
        "                print(f\"Warning: AI Summarizer failed to load: {e}\")\n",
        "                self.summarizer = None\n",
        "\n",
        "    def summarize(self, text):\n",
        "        if self.summarizer:\n",
        "            try:\n",
        "                # Limit input size for the summarizer\n",
        "                input_text = text[:4000]\n",
        "                result = self.summarizer(input_text, max_length=150, min_length=50, do_sample=False)\n",
        "                return result[0]['summary_text']\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: AI summarization failed: {e}\")\n",
        "                pass\n",
        "\n",
        "        # Fallback to extractive summary (first few sentences)\n",
        "        sentences = sent_tokenize(text)\n",
        "        return \" \".join(sentences[:5])\n",
        "\n",
        "# Main Documentation Generator\n",
        "class DocumentationGeneratorModel:\n",
        "    def __init__(self, api_key):\n",
        "        self.scaledown = ScaleDownAPI(api_key)\n",
        "        self.structure = StructureModel()\n",
        "        self.explainer = ExplanationModel()\n",
        "        self.summarizer = SummaryModel()\n",
        "\n",
        "    def generate(self, text):\n",
        "        # Try to compress, fallback to original if it fails\n",
        "        processed_text = self.scaledown.compress_text(text)\n",
        "\n",
        "        structure = self.structure.extract(text)\n",
        "        simplified = self.explainer.simplify(processed_text)\n",
        "        summary = self.summarizer.summarize(processed_text)\n",
        "\n",
        "        return {\"structure\": structure, \"summary\": summary, \"easy_text\": simplified}"
      ],
      "metadata": {
        "id": "5HR3ysCASB7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "ext = filename.split('.')[-1].lower()\n",
        "\n",
        "text_document = \"\"\n",
        "\n",
        "if ext == \"txt\":\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        text_document = f.read()\n",
        "elif ext == \"pdf\":\n",
        "    # Try pdfplumber first\n",
        "    with pdfplumber.open(filename) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text_document += page_text + \"\\n\"\n",
        "\n",
        "    # Fallback to pypdf if pdfplumber fails to extract text\n",
        "    if not text_document.strip():\n",
        "        reader = pypdf.PdfReader(filename)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_document += text + \"\\n\"\n",
        "elif ext == \"docx\":\n",
        "    doc = Document(filename)\n",
        "    text_document = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "else:\n",
        "    raise ValueError(\"Unsupported file type! Please upload TXT, PDF, or DOCX.\")\n",
        "\n",
        "if text_document.strip():\n",
        "    print(f\"Successfully extracted text from {filename}!\")\n",
        "    print(text_document[:500], \"...\\n\")\n",
        "else:\n",
        "    print(f\"Warning: Could not extract selectable text from {filename}. The file might be a scan or images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "S-l5321kSGDY",
        "outputId": "25307341-0752-4dd8-dab5-7c453317843b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b843ab1-4151-4e83-b799-a9a21568df64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b843ab1-4151-4e83-b799-a9a21568df64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AHNA ALM.pdf to AHNA ALM.pdf\n",
            "Successfully extracted text from AHNA ALM.pdf!\n",
            "Invention Disclosure Form\n",
            "1. Title of the Invention\n",
            "AHNA (Audio Hearing and Neural Analysis)- Intelligent Audio Understanding Model Integra8ng\n",
            "ASR, Diariza8on, Event Detec8on, Paralinguis8cs, and Reasoning for Edge and SoCware\n",
            "Deployment.\n",
            "2. Background of the Invention (i.e., Known Prior Arts)\n",
            "Over the past decade, significant advancements have been made in audio intelligence, primarily\n",
            "driven by deep learning and mul8modal language models.\n",
            "Despite these advancements, most exis8ng approaches foc ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to prevent UnicodeEncodeError in the next cell (PDF generation)\n",
        "def clean_for_pdf(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    replacements = {\n",
        "        \"\\ufb01\": \"fi\", \"\\ufb02\": \"fl\", \"\\ufb03\": \"ffi\", \"\\ufb04\": \"ffl\",\n",
        "        \"\\u201c\": '\"', \"\\u201d\": '\"', \"\\u2018\": \"'\", \"\\u2019\": \"'\",\n",
        "        \"\\u2013\": \"-\", \"\\u2014\": \"-\", \"\\u2022\": \"*\"\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "    return text.encode('latin-1', 'ignore').decode('latin-1')\n",
        "\n",
        "# Run the generator\n",
        "if not text_document.strip():\n",
        "    print(\"Error: No text found to process. Please upload a document with selectable text.\")\n",
        "    summary = \"N/A\"\n",
        "    structure = \"N/A\"\n",
        "    easy_text = \"N/A\"\n",
        "else:\n",
        "    api_key = \"s2ooL68SevahKqZrYiKwy8ijOMxqoP4Yag4LFzIA\"\n",
        "    model = DocumentationGeneratorModel(api_key)\n",
        "    output = model.generate(text_document)\n",
        "\n",
        "    # Clean the outputs so they are compatible with FPDF\n",
        "    summary = clean_for_pdf(output[\"summary\"])\n",
        "    structure = clean_for_pdf(\"\\n\".join(output[\"structure\"]))\n",
        "    easy_text = clean_for_pdf(output[\"easy_text\"])\n",
        "\n",
        "    print(\"----- SUMMARY -----\")\n",
        "    print(summary)\n",
        "    print(\"\\n----- STRUCTURE -----\")\n",
        "    print(structure)\n",
        "    print(\"\\n----- EASY VERSION -----\")\n",
        "    print(easy_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq-5Up0lSICu",
        "outputId": "65aa39c0-357d-45f7-8730-eaadfa86d4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: AI Summarizer failed to load: \"Unknown task summarization, available tasks are ['any-to-any', 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'token-classification', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"\n",
            "Warning: ScaleDown Compression failed: ('ScaleDown API failed:', {'detail': 'All 1 compression requests failed'})\n",
            "----- SUMMARY -----\n",
            "Invention Disclosure Form\n",
            "1. Title of the Invention\n",
            "AHNA (Audio Hearing and Neural Analysis)- Intelligent Audio Understanding Model Integra8ng\n",
            "ASR, Diariza8on, Event Detec8on, Paralinguis8cs, and Reasoning for Edge and SoCware\n",
            "Deployment. 2. Background of the Invention (i.e., Known Prior Arts)\n",
            "Over the past decade, significant advancements have been made in audio intelligence, primarily\n",
            "driven by deep learning and mul8modal language models. Despite these advancements, most exis8ng approaches focus on individual audio subtasks in\n",
            "isola8on, such as Automa8c Speech Recogni8on (ASR), speaker diariza8on, audio event\n",
            "detec8on, or paralinguis8c/emo8on analysis, without unified contextual reasoning or efficient\n",
            "deployment on edge devices.\n",
            "\n",
            "----- STRUCTURE -----\n",
            "They lack:\n",
            "audio understanding system. Such a system would:\n",
            "that dis8nguish it from exis8ng audio understanding technologies:\n",
            "Unified Mul.task Audio Understanding:\n",
            "Context-Aware Audio Reasoning :\n",
            "Parallel Mul.modal Processing Architecture :\n",
            "Edge-Op.mized Deployment :\n",
            "Adap.ve and Explainable Outputs :\n",
            "Scalable Offline and Real-Time Opera3on :\n",
            "Comprehensive Fusion of Audio Modali.es :\n",
            "4.1. Audio Chunking Module :\n",
            "4.2. Parallel Processing Units :\n",
            "The architecture employs four primary, concurrent processors:\n",
            "ASR Processor :\n",
            "Example output:\n",
            "Diariza8on Processor :\n",
            "Event Detec8on Processor :\n",
            "Paralinguis8c Processor :\n",
            "4.3. Fusion Engine :\n",
            "Fusion methods may include:\n",
            "4.4. Reasoner Module :\n",
            "Finaliza8on is triggered by :\n",
            "5. Workflow Summary :\n",
            "6. Implementa.on and Deployment :\n",
            "\n",
            "----- EASY VERSION -----\n",
            "Invention Disclosure Form\n",
            "1. Title of the Invention\n",
            "AHNA (Audio Hearing and Neural Analysis)- Intelligent Audio Understanding Model Integra8ng\n",
            "ASR, Diariza8on, Event Detec8on, Paralinguis8cs, and Reasoning for Edge and SoCware\n",
            "Deployment.\n",
            "2. Background of the Invention (i.e., Known Prior Arts)\n",
            "Over the past decade, significant advancements have been made in audio intelligence, primarily\n",
            "driven by deep learning and mul8modal language models.\n",
            "Despite these advancements, most exis8ng approaches focus on individual audio subtasks in\n",
            "isola8on, such as Automa8c Speech Recogni8on (ASR), speaker diariza8on, audio event\n",
            "detec8on, or paralinguis8c/emo8on analysis, without unified contextual reasoning or efficient\n",
            "deployment on edge devices.\n",
            "Prior Arts and Limita.ons\n",
            "(a) Automa.c Speech Recogni.on (ASR) Systems\n",
            "Models such as Whisper (OpenAI, 2022), DeepSpeech (Mozilla, 2017), and Wav2Vec2.0 (Meta AI,\n",
            "2020) achieve high transcrip8on accuracy across mul8ple languages and acous8c condi8ons.\n",
            "Limita8ons: While robust in speech-to-text conversion, these models are task-specific.\n",
            "They lack:\n",
            "Integra8on with event or speaker context.\n",
            "Emo8onal and paralinguis8c understanding.\n",
            "Reasoning over non-speech audio.\n",
            "(b) Speaker Diariza.on and Iden.fica.on Models\n",
            "Conven8onal systems (x-vectors, ECAPA-TDNN, pyannote.audio) excel at segmen8ng and\n",
            "labeling \"who spoke when.\"\n",
            "Limita8ons: These models operate independently of the seman8c or emo8onal content of\n",
            "speech and do not integrate with other auditory modali8es for unified reasoning.\n",
            "(c) Audio Event and Scene Detec.on\n",
            "Models like AudioSet, YAMNet, and PANNs (Pretrained Audio Neural Networks) have advanced\n",
            "environmental sound classifica8on and event tagging.\n",
            "Limita8ons: These systems are generally sta8c classifiers, lack cross-domain reasoning (linking\n",
            "events to speech intent), and are typically cloud-based, making edge deployment challenging.\n",
            "(d) Paralinguis.c and Emo.onal Speech Models\n",
            "Systems such as OpenSMILE, DeepSpectrum, and SERNet extract prosody, pitch, emo8on, and\n",
            "voice-quality features.\n",
            "Limita8ons: While effec8ve in controlled environments, these models struggle with overlapping\n",
            "or noisy speech, and rarely connect to reasoning frameworks for decision-level interpreta8on.\n",
            "(e) Mul.modal Audio-Language Models (LALMs)\n",
            "Recent models, including Audio Flamingo (NVIDIA, 2024), Qwen-Audio (Alibaba, 2024), Pengi\n",
            "(Google, 2023), and LTU (2024), aaempt to integrate audio and text for tasks like cap8oning,\n",
            "ques8on answering, and dialogue.\n",
            "Limita8ons: These models are computa8onally intensive, unsuitable for real-8me or edge\n",
            "applica8ons, do not fully integrate ASR, diariza8on, and paralinguis8cs into a single unified\n",
            "pipeline, and cannot reason over overlapping or concurrent sound sources effec8vely.\n",
            "Iden.fied Gaps in Prior Arts\n",
            "Fragmenta8on of Audio Subtasks: Most systems process ASR, speaker recogni8on, event\n",
            "detec8on, and paralinguis8cs independently, without a unified reasoning framework.\n",
            "Lack of Contextual Reasoning: Exis8ng models perform classifica8on or transcrip8on but do not\n",
            "understand rela8onships among mul8ple auditory cues (e.g., linking tone, speaker iden8ty, and\n",
            "environmental sound).\n",
            "Inefficiency for Edge Deployment: High-performing models rely on large transformer\n",
            "architectures, making low-latency, on-device processing infeasible.\n",
            "Absence of Unified Mul8task Intelligence: No exis8ng solu8on fully integrates speech, event,\n",
            "emo8on, and reasoning into a cohesive pipeline that can scale from edge devices to soCware\n",
            "placorms.\n",
            "Need for the Inven.on\n",
            "These limita8ons create a strong demand for an integrated, efficient, and contextually intelligent\n",
            "audio understanding system. Such a system would:\n",
            "(a) Combine speech recogni8on, speaker tracking, event and emo8on detec8on, and reasoning\n",
            "in a unified neural architecture.\n",
            "(b) Achieve real-8me performance with low computa8onal overhead suitable for embedded\n",
            "devices.\n",
            "(c) Provide explainable outputs and adapt to diverse auditory scenes and mul8-speaker\n",
            "interac8ons.\n",
            "3. Novelty of the Invention\n",
            "The AHNA (Audio Hearing and Neural Analysis) system introduces several novel contribu8ons\n",
            "that dis8nguish it from exis8ng audio understanding technologies:\n",
            "Unified Mul.task Audio Understanding:\n",
            "Unlike prior systems that handle Automa8c Speech Recogni8on (ASR), speaker diariza8on, audio\n",
            "event detec8on, and paralinguis8cs in isola8on, AHNA integrates all these modali8es within a\n",
            "single unified neural pipeline. This enables simultaneous reasoning across speech, speaker\n",
            "iden8ty, environmental sounds, and emo8onal or prosodic cues, providing a comprehensive\n",
            "understanding of complex auditory scenes.\n",
            "Context-Aware Audio Reasoning :\n",
            "AHNA incorporates a reasoning layer that performs context-aware inference over concurrent or\n",
            "overlapping sound sources. This allows the system to not only classify or transcribe audio but\n",
            "also understand rela8onships among mul8ple auditory cues, such as linking tone, speaker intent,\n",
            "and environmental events - a capability largely absent in prior arts.\n",
            "Parallel Mul.modal Processing Architecture :\n",
            "The system uses parallel processors for ASR, diariza8on, event detec8on, and paralinguis8cs,\n",
            "which run simultaneously on raw audio input. This parallelism enables high efficiency and\n",
            "reduces latency, allowing near-real-8me processing suitable for edge devices - an improvement\n",
            "over sequen8al or cloud-dependent architectures in exis8ng models.\n",
            "Edge-Op.mized Deployment :\n",
            "AHNA employs lightweight transformer compression, hierarchical aaen8on, and quan8za8on\n",
            "techniques, enabling deployment on low-power edge devices while maintaining high inference\n",
            "accuracy. This contrasts with prior audio-language models that are resource-intensive and\n",
            "typically cloud-dependent.\n",
            "Adap.ve and Explainable Outputs :\n",
            "The system produces interpretable reasoning outputs for mul8-speaker dialogues, overlapping\n",
            "events, and paralinguis8c cues, enhancing transparency and adaptability in real-world\n",
            "applica8ons. Exis8ng systems generally lack explainability or adaptability for dynamic auditory\n",
            "environments.\n",
            "Scalable Offline and Real-Time Opera3on :\n",
            "AHNA is designed for both offline and real-8me inference, allowing seamless integra8on into\n",
            "soCware placorms, IoT systems, and assis8ve AI devices. This dual-mode opera8on is a unique\n",
            "feature, as most high-performing audio-language models are constrained to offline or cloud-only\n",
            "deployment.\n",
            "Comprehensive Fusion of Audio Modali.es :\n",
            "Through a mul8modal fusion mechanism, AHNA effec8vely combines diverse auditory inputs\n",
            "into a cohesive feature space, enabling more accurate predic8ons and higher-level reasoning\n",
            "compared to single-task models or LALMs that fuse only limited modali8es.\n",
            "In summary, the novelty of AHNA lies in its ability to unify mul8ple audio processing tasks,\n",
            "reason contextually over overlapping and complex auditory inputs, operate efficiently on edge\n",
            "devices, provide explainable outputs, and support scalable deployment - capabili8es that are\n",
            "not collec8vely addressed by any known prior art.\n",
            "4. Detailed Description of the Invention\n",
            "Workflow Diagram\n",
            "4.1. Audio Chunking Module :\n",
            "The system captures a con8nuous audio stream at a sample rate of 16 kHz or 24 kHz.\n",
            "The stream is segmented into overlapping audio chunks of 300 milliseconds each, with a hop size\n",
            "of 120 milliseconds and a lookahead buffer of 500 milliseconds.\n",
            "The lookahead mechanism appends future samples to each chunk, improving temporal context\n",
            "for mid-sentence understanding without introducing excessive delay.\n",
            "Each chunk is 8me-stamped rela8ve to the start of the stream and dispatched to all processing\n",
            "units in parallel.\n",
            "4.2. Parallel Processing Units :\n",
            "The architecture employs four primary, concurrent processors:\n",
            "ASR Processor :\n",
            "Converts audio chunks into phoneme or token-level text with precise start and end 8mestamps.\n",
            "Example output:\n",
            "{\n",
            "\"processor\": \"ASRProcessor\",\n",
            "\"start_8me\": 12.45,\n",
            "\"end_8me\": 12.75,\n",
            "\"payload\": {\"text\": \"Turn on the light\", \"tokens\": [...]},\n",
            "\"confidence\": 0.92\n",
            "}\n",
            "Diariza8on Processor :\n",
            "Detects speaker iden88es and boundaries across overlapping speech segments.\n",
            "{\"segments\": [{\"start\": 12.45, \"end\": 12.75, \"speaker\": \"spk_1\"}]}\n",
            "Event Detec8on Processor :\n",
            "Recognizes non-speech acous8c events such as door knocks, typing, or ambient signals.\n",
            "{\"events\": [{\"label\": \"door_open\", \"8me\": 12.62, \"score\": 0.91}]}\n",
            "Paralinguis8c Processor :\n",
            "Analyzes voice aaributes like pitch, energy, and emo8on to infer speaker affect and tone.\n",
            "{\"pitch\": 120, \"energy\": 0.34, \"emo8on\": \"neutral\"}\n",
            "Each processor emits outputs with consistent temporal references and metadata, ensuring\n",
            "alignment at the fusion stage.\n",
            "4.3. Fusion Engine :\n",
            "The Fusion Engine is a 8me-synchronized integra8on layer that aggregates and aligns outputs\n",
            "from all processors using a common 8me base.\n",
            "It merges text tokens, speaker informa8on, detected events, and paralinguis8c cues into a\n",
            "unified mul8modal data structure.\n",
            "Fusion methods may include:\n",
            "(a) Token-level enrichment: augmen8ng ASR tokens with speaker, event, and emo8on metadata.\n",
            "(b) Frame-level concatena8on: combining numeric feature vectors for temporal correla8on.\n",
            "(c) Confidence-weighted merging: priori8zing high-confidence processor outputs.\n",
            "The fused output is formaaed for downstream reasoning and contains seman8cally enriched,\n",
            "temporally aligned frames.\n",
            "4.4. Reasoner Module :\n",
            "The Reasoner operates in streaming mode, maintaining internal state to track context across\n",
            "incoming fused segments.\n",
            "It performs incremental reasoning to generate par8al summaries during speech and final\n",
            "summaries at uaerance comple8on.\n",
            "Finaliza8on is triggered by :\n",
            "Detec8on of speech silence exceeding a threshold (e.g., 500-800 ms).\n",
            "Explicit end-of-uaerance markers.\n",
            "The Reasoner's output evolves dynamically, revising par8al hypotheses into stable, high-\n",
            "confidence summaries once finaliza8on occurs.\n",
            "5. Workflow Summary :\n",
            "Audio input is con8nuously captured and segmented into overlapping chunks with lookahead.\n",
            "Each chunk is processed concurrently by ASR, Diariza8on, Event, and Paralinguis8c modules.\n",
            "All processor outputs are 8mestamped and sent to the Fusion Engine.\n",
            "The Fusion Engine synchronizes outputs and produces enriched representa8ons.\n",
            "The Reasoner consumes fused data incrementally to generate par8al summaries.\n",
            "Upon uaerance comple8on, the Reasoner generates a comprehensive final summary.\n",
            "This workflow ensures real-8me adaptability, temporal precision, and context-aware mul8modal\n",
            "understanding.\n",
            "6. Implementa.on and Deployment :\n",
            "The inven8on can be implemented in Python with asynchronous event-driven architectures\n",
            "(e.g., asyncio, message queues).\n",
            "Models may be deployed via ONNX, TFLite, or TensorRT for edge devices, with cloud offloading\n",
            "for heavier reasoning tasks.\n",
            "The system can further incorporate a circular buffer mechanism for con8nuous audio handling,\n",
            "bounded queues for backpressure management, and state persistence for con8nuity across\n",
            "chunks.\n",
            "5. Advantages of the Invention\n",
            "(a) Low Latency: Lookahead buffering enables fast, context-aware inference without wai8ng for\n",
            "full uaerance comple8on.\n",
            "(b) Parallel Modularity: Each processor operates independently, allowing scalability and\n",
            "hardware op8miza8on.\n",
            "(c) Temporal Fusion: Consistent 8mestamping ensures accurate alignment of mul8modal signals.\n",
            "(d) Incremental Reasoning: The Reasoner provides both immediate feedback (par8al summaries)\n",
            "and refined outputs (final summaries).\n",
            "(e) Versa8lity: The framework supports both on-device and cloud-deployed models, enabling\n",
            "flexible deployment on edge systems.\n",
            "(f) Robustness: Confidence-based fusion mi8gates individual processor errors, enhancing system\n",
            "reliability.\n",
            "(g) Emotionally aware: By embedding emotion detection, the system can adapt its tone and\n",
            "follow-up strategy to the user's affective state.\n",
            "(f) Acoustic context preserved: Because the system retains prosodic and acoustic cues, it can\n",
            "identify hesitation, sarcasm, uncertainty, etc., which purely text-based systems cannot reliably\n",
            "capture.\n",
            "(g) Multilingual and Code-Switching Adaptability:The invention is capable of recognizing and\n",
            "processing multiple languages and dialects within a single interaction, adapting automatically to\n",
            "language shifts and accents.\n",
            "(h) Unified End-to-End Architecture:The system integrates speech detection, emotion\n",
            "recognition, reasoning, question generation, and knowledge retrieval into a single end-to-end\n",
            "framework, reducing complexity and error propagation.\n",
            "(i) Speech and Non-Speech Differentiation:The model accurately differentiates between speech,\n",
            "silence, noise, and other non-speech sounds, ensuring reliable operation in real-world audio\n",
            "environments.\n",
            "6. Application Area of the Invention\n",
            "1. Smart Assistants: Real-8me understanding of commands and emo8ons for responsive AI\n",
            "interac8on.\n",
            "2. Mee8ng & Call Analy8cs: Speaker-aware transcrip8ons and automa8c summaries of\n",
            "conversa8ons.\n",
            "3. Surveillance & Security: Detect cri8cal acous8c events and monitor environments in real 8me.\n",
            "4. Healthcare & Therapy: Track speech paaerns and emo8onal tone for therapy and health\n",
            "monitoring.\n",
            "5. Human-Robot Interac8on: Enable robots to understand commands, context, and emo8ons\n",
            "instantly.\n",
            "6. Media Monitoring & Broadcast Analysis: Analyze live audio streams for speakers, events, and\n",
            "emo8onal cues.\n",
            "7.Customer Support and Call Centers: The system can analyze customer speech, detect emo8ons\n",
            "such as frustra8on or sa8sfac8on, and guide agents or automated bots to provide more effec8ve\n",
            "and empathe8c responses.\n",
            "8.Legal and Forensic Audio Analysis: The inven8on can aid in analyzing recorded speech for\n",
            "stress, decep8on, or emo8onal indicators, suppor8ng inves8ga8ons or courtroom analysis.\n",
            "9.Emergency Response and Disaster Management: The inven8on can assist in analyzing distress\n",
            "calls or field communica8ons in real 8me, iden8fying emo8onal urgency and guiding rapid,\n",
            "informed decision-making.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = FPDF()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", 'B', 16)\n",
        "pdf.cell(0, 10, \"Documentation Output\", ln=True, align='C')\n",
        "pdf.ln(10)\n",
        "\n",
        "pdf.set_font(\"Arial\", 'B', 14)\n",
        "pdf.cell(0, 10, \"Summary:\", ln=True)\n",
        "pdf.set_font(\"Arial\", '', 12)\n",
        "pdf.multi_cell(0, 8, summary)\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", 'B', 14)\n",
        "pdf.cell(0, 10, \"Structure:\", ln=True)\n",
        "pdf.set_font(\"Arial\", '', 12)\n",
        "pdf.multi_cell(0, 8, structure)\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", 'B', 14)\n",
        "pdf.cell(0, 10, \"Easy Text:\", ln=True)\n",
        "pdf.set_font(\"Arial\", '', 12)\n",
        "pdf.multi_cell(0, 8, easy_text)\n",
        "\n",
        "output_filename = \"documentation_output.pdf\"\n",
        "pdf.output(output_filename)\n",
        "print(f\"PDF saved as {output_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ6bCNwZSJjn",
        "outputId": "27ba3695-2c35-4ed8-883b-722ee6b50f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF saved as documentation_output.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rG6IWlSsSMDr",
        "outputId": "d6bdec4c-19a2-4778-e583-95e030908c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d7449965-0a65-429a-bd2d-d401a514ff05\", \"documentation_output.pdf\", 12829)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GITHUB_USERNAME = \"Jainkavya29\"\n",
        "GITHUB_REPO = \"Documentation-Generator-Agent\"\n",
        "GITHUB_TOKEN = \"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\"  # <-- Replace with your new token\n",
        "\n",
        "# Configure Git\n",
        "!git config --global user.name \"Jainkavya29\"\n",
        "!git config --global user.email \"kavyajain292005@gmail.com\"\n",
        "\n",
        "# Initialize repo if not exists\n",
        "if not os.path.exists(\".git\"):\n",
        "    !git init\n",
        "\n",
        "# Stage all files\n",
        "!git add .\n",
        "\n",
        "# Commit changes\n",
        "!git commit -m \"Update project files\" || print(\"Nothing to commit\")\n",
        "\n",
        "# Remove old remote if exists\n",
        "!git remote remove origin || echo \"No remote to remove\"\n",
        "!git remote add origin https://Jainkavya29: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@github.com/Jainkavya29/Documentation-Generator-Agent.git\n",
        "\n",
        "\n",
        "\n",
        "# Push to main branch\n",
        "!git branch -M main\n",
        "!git push -u origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iimSCfhgCgP",
        "outputId": "9f2027cb-4dcf-4f85-f0b6-e72146e025c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `\"Nothing to commit\"'\n",
            "/bin/bash: -c: line 1: `git commit -m \"Update project files\" || print(\"Nothing to commit\")'\n",
            "Enumerating objects: 31, done.\n",
            "Counting objects: 100% (31/31), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (24/24), done.\n",
            "Writing objects: 100% (31/31), 60.27 MiB | 7.61 MiB/s, done.\n",
            "Total 31 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), done.\u001b[K\n",
            "To https://github.com/Jainkavya29/Documentation-Generator-Agent.git\n",
            " * [new branch]      main -> main\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        }
      ]
    }
  ]
}